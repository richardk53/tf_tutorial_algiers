{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - keras, distributions, optimizers, tensorboard, saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented and trained a model from scratch, we are ready introduce a selected list of useful higher-level API and utilities that make your life easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams & constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in, dim_out = 784, 10\n",
    "dims_hidden = [128, 64]\n",
    "activations_hidden = [\"relu\", \"relu\"]\n",
    "batch_size = 128\n",
    "learning_rate = 5e-4\n",
    "\n",
    "num_iterations_train = 20000\n",
    "\n",
    "now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "log_path = os.path.join(\"logs\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(log_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-f6adce4340e6>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/Richard/.virtualenvs/tf_tutorial/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/Richard/.virtualenvs/tf_tutorial/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/Richard/.virtualenvs/tf_tutorial/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/Richard/.virtualenvs/tf_tutorial/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# This time we will use integer labels directly instead of one_hot. \n",
    "data = input_data.read_data_sets(\"data/MNIST/\", one_hot=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - TODO: Make tf keras Model with print? and tf.summary_writer (is probably automatically in keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \"\"\" Multi-Layer-Perceptron \"\"\"\n",
    "    def __init__(self, name: str, dim_in: int, dims: tuple, activations: tuple):\n",
    "        self.name = name\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dims[-1]\n",
    "        self.shp_in = (dim_in,)\n",
    "        self.shp_out = (dims[-1],)\n",
    "        \n",
    "        dims_in = (dim_in,) + tuple(dims[:-1])\n",
    "        dims_out = dims\n",
    "        self.layers = list()\n",
    "        with tf.variable_scope(name, reuse=False): \n",
    "            for idx_hidden, (dim_in, dim_out, activation) in enumerate(zip(dims_in, dims_out, activations)):\n",
    "                kernel_initializer, bias_initializer = self.get_initializers_for(activation=activation)\n",
    "                # ***********************************\n",
    "                layer = tf.keras.layers.Dense(dim_out, \n",
    "                                              activation=activation, \n",
    "                                              name=\"layer_{}\".format(idx_hidden), \n",
    "                                              kernel_initializer=kernel_initializer, \n",
    "                                              bias_initializer=bias_initializer,\n",
    "                                             )\n",
    "                layer.build(dim_in)\n",
    "                # ***********************************\n",
    "                self.layers.append(layer)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        return h\n",
    "    \n",
    "    def get_initializers_for(self, activation: str, distribution: str = \"uniform\", mode : str = \"fan_in\"):\n",
    "        \"\"\" Helper function to choose appropriate initialization method, depending on the activation function. \"\"\"\n",
    "        if not isinstance(activation, str):\n",
    "            activation = activation.__name__\n",
    "\n",
    "        if activation in [None, 'linear']:\n",
    "            scale = 1.0\n",
    "        elif activation is \"relu\":\n",
    "            scale = 2.0  \n",
    "        elif activation is \"tanh\":\n",
    "            scale = 1.32\n",
    "        else:\n",
    "            raise ValueError(\"unexpected activation function: {}\".format(activation))\n",
    "\n",
    "        kernel_initializer = tf.initializers.variance_scaling(\n",
    "            scale=scale,\n",
    "            mode=mode,\n",
    "            distribution=distribution,\n",
    "        )\n",
    "        bias_initializer = tf.initializers.constant(0.0)\n",
    "        return kernel_initializer, bias_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()  # Might be helpful, if you rebuild your model.\n",
    "\n",
    "# Batch size None means that batch size is define by the data provided by the feed_dict to session.run()\n",
    "labels = tf.placeholder(shape=[None], dtype=tf.int32, name=\"targets\")  \n",
    "inputs = tf.placeholder(shape=[None, dim_in], dtype=tf.float32, name=\"inputs\")\n",
    "model = MLP(name=\"MLP\", \n",
    "            dim_in=dim_in, \n",
    "            dims=dims_hidden + [dim_out],  # last dimension is for output layer.\n",
    "            activations=activations_hidden + [\"linear\"],  # Output has no activation function.\n",
    "           )\n",
    "logits = model(inputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** I strongly recommend to use tf.distributions. \n",
    "This way, you are explicit about your assumptions about the data distribution. \n",
    "Furthermore, your loss function directly follows from this assumption, i.e. your objective is to maximize the log-likelihood $\\log p_{\\theta}(y ~|~ x)$, where $\\theta$ are the model parameters (variables in tensorflow). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logits, labels):\n",
    "    with tf.name_scope(\"accuracy\"): \n",
    "        correct_prediction = tf.equal(tf.cast(tf.argmax(logits, axis=-1), tf.int32), labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def get_loglikelihood(logits, labels):\n",
    "#     with tf.name_scope(\"likelihood_distribution\"):\n",
    "    likelihood_dist = tf.distributions.Categorical(  # tf.contrib.distributions.OneHotCategorical\n",
    "        logits=logits, name=\"predicted_labels\")  \n",
    "    with tf.name_scope(\"log-likelihood\"):\n",
    "        loglikelihood = tf.reduce_sum(likelihood_dist.log_prob(labels))  \n",
    "    return loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = - get_loglikelihood(logits, labels)\n",
    "accuracy = get_accuracy(logits, labels)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect model on tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize the graph on Tensorboard, we must add it to the summaries.\n",
    "summary_writer = tf.summary.FileWriter(log_path)\n",
    "summary_writer.add_graph(tf.get_default_graph())\n",
    " \n",
    "# We can do the same in one line, by passing the graph to the constructor of the FileWriter\n",
    "# tf.summary.FileWriter(log_path, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph collections, BEFORE we added our summaries: [('__variable_store',), ('__varscope',), 'trainable_variables', 'variables', 'update_ops', 'train_op']\n",
      "Graph collections, AFTER we added our summaries: [('__variable_store',), ('__varscope',), 'trainable_variables', 'variables', 'update_ops', 'train_op', 'validation_summaries']\n",
      "We should have a summary key: validation_summaries\n"
     ]
    }
   ],
   "source": [
    "# We can also add some other useful statistics to tensorboard and log them periodically (during training).\n",
    "# We do this by creating a unique \"summary_key\" (just a string), which we add to the graph collections.\n",
    "print(\"Graph collections, BEFORE we added our summaries: {}\".format(\n",
    "    tf.get_default_graph().collections))\n",
    "validation_summary_key = tf.get_default_graph().unique_name(\"validation_summaries\")\n",
    "\n",
    "# Add loss to our summaries key in the graph collections\n",
    "tf.summary.scalar(\"loss\", loss, collections=[validation_summary_key])\n",
    "\n",
    "# Let's also add some gradient statistics. This is often useful for debugging\n",
    "variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "gradients = tf.gradients(loss, variables)\n",
    "for var, grad in zip(variables, gradients):\n",
    "    tf.summary.histogram(name=\"gradient/\" + var.name.replace(':', '/'), \n",
    "                         values=grad, \n",
    "                         collections=[validation_summary_key])\n",
    "    tf.summary.scalar(name=\"gradient_norm/\" + var.name.replace(':', '/'),\n",
    "                      tensor=tf.norm(grad), \n",
    "                      collections=[validation_summary_key])\n",
    "    \n",
    "    tf.summary.histogram(name=\"parameter/\" + var.name.replace(':', '/'), \n",
    "                         values=var, \n",
    "                         collections=[validation_summary_key])\n",
    "    tf.summary.scalar(name=\"parameter_norm/\" + var.name.replace(':', '/'),\n",
    "                      tensor=tf.norm(var), \n",
    "                      collections=[validation_summary_key])\n",
    "\n",
    "# Let's make one summary, that we can run with as a single operation. (summarize is an operation)\n",
    "validation_summaries = tf.summary.merge_all(key=validation_summary_key)\n",
    "\n",
    "print(\"Graph collections, AFTER we added our summaries: {}\".format(\n",
    "    tf.get_default_graph().collections))\n",
    "print(\"We should have a summary key: {}\".format(validation_summary_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training\n",
    "# # ema = tf.train.ExponentialMovingAverage(decay=0.98, zero_debias=True)\n",
    "# # ema.apply([loss])\n",
    "\n",
    "# tf.metrics.mean(\n",
    "#     values,\n",
    "#     metrics_collections=None,\n",
    "#     updates_collections=None,\n",
    "#     name=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your terminal do: \n",
    "<br>\n",
    "tensorboard --logdir=PATH_TO_LOG_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) # Initializes weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 / 20000, validation accuracy = 0.09700000286102295, loss = 11675.59375\n",
      "iter 500 / 20000, validation accuracy = 0.9435999989509583, loss = 1002.8472900390625\n",
      "iter 1000 / 20000, validation accuracy = 0.9595999717712402, loss = 702.9844970703125\n",
      "iter 1500 / 20000, validation accuracy = 0.9649999737739563, loss = 584.0706787109375\n",
      "iter 2000 / 20000, validation accuracy = 0.9710000157356262, loss = 499.24322509765625\n",
      "iter 2500 / 20000, validation accuracy = 0.9739999771118164, loss = 458.9294738769531\n",
      "iter 3000 / 20000, validation accuracy = 0.9721999764442444, loss = 429.9996337890625\n",
      "iter 3500 / 20000, validation accuracy = 0.978600025177002, loss = 380.74542236328125\n",
      "iter 4000 / 20000, validation accuracy = 0.9771999716758728, loss = 386.56158447265625\n",
      "iter 4500 / 20000, validation accuracy = 0.9782000184059143, loss = 382.8121032714844\n",
      "iter 5000 / 20000, validation accuracy = 0.9787999987602234, loss = 369.7127990722656\n",
      "iter 5500 / 20000, validation accuracy = 0.9782000184059143, loss = 402.68377685546875\n",
      "iter 6000 / 20000, validation accuracy = 0.9782000184059143, loss = 392.97393798828125\n",
      "iter 6500 / 20000, validation accuracy = 0.978600025177002, loss = 379.6578674316406\n",
      "iter 7000 / 20000, validation accuracy = 0.9797999858856201, loss = 357.2008056640625\n",
      "iter 7500 / 20000, validation accuracy = 0.9797999858856201, loss = 378.1615295410156\n",
      "iter 8000 / 20000, validation accuracy = 0.9800000190734863, loss = 387.68536376953125\n",
      "iter 8500 / 20000, validation accuracy = 0.978600025177002, loss = 415.15423583984375\n",
      "iter 9000 / 20000, validation accuracy = 0.9765999913215637, loss = 441.7141418457031\n",
      "iter 9500 / 20000, validation accuracy = 0.9782000184059143, loss = 444.6322021484375\n",
      "iter 10000 / 20000, validation accuracy = 0.978600025177002, loss = 408.62939453125\n",
      "iter 10500 / 20000, validation accuracy = 0.9775999784469604, loss = 454.7400817871094\n",
      "iter 11000 / 20000, validation accuracy = 0.979200005531311, loss = 438.8720703125\n",
      "iter 11500 / 20000, validation accuracy = 0.9797999858856201, loss = 443.08050537109375\n",
      "iter 12000 / 20000, validation accuracy = 0.980400025844574, loss = 436.0693359375\n",
      "iter 12500 / 20000, validation accuracy = 0.9797999858856201, loss = 440.4444885253906\n",
      "iter 13000 / 20000, validation accuracy = 0.9778000116348267, loss = 514.849365234375\n",
      "iter 13500 / 20000, validation accuracy = 0.9811999797821045, loss = 438.1002197265625\n",
      "iter 14000 / 20000, validation accuracy = 0.9793999791145325, loss = 514.764404296875\n",
      "iter 14500 / 20000, validation accuracy = 0.9797999858856201, loss = 500.712646484375\n",
      "iter 15000 / 20000, validation accuracy = 0.979200005531311, loss = 522.2012329101562\n",
      "iter 15500 / 20000, validation accuracy = 0.9779999852180481, loss = 545.0238037109375\n",
      "iter 16000 / 20000, validation accuracy = 0.980400025844574, loss = 494.2090759277344\n",
      "iter 16500 / 20000, validation accuracy = 0.9815999865531921, loss = 488.18548583984375\n",
      "iter 17000 / 20000, validation accuracy = 0.9805999994277954, loss = 495.80914306640625\n",
      "iter 17500 / 20000, validation accuracy = 0.9810000061988831, loss = 512.6156005859375\n",
      "iter 18000 / 20000, validation accuracy = 0.9811999797821045, loss = 503.014892578125\n",
      "iter 18500 / 20000, validation accuracy = 0.9807999730110168, loss = 512.4841918945312\n",
      "iter 19000 / 20000, validation accuracy = 0.978600025177002, loss = 565.2503662109375\n",
      "iter 19500 / 20000, validation accuracy = 0.9814000129699707, loss = 489.58148193359375\n",
      "iter 19999 / 20000, validation accuracy = 0.9811999797821045, loss = 493.7310791015625\n"
     ]
    }
   ],
   "source": [
    "best_val_accuracy = -np.inf\n",
    "for iter_train in range(num_iterations_train):  \n",
    "    batch_inputs, batch_labels = data.train.next_batch(batch_size=batch_size, shuffle=True)\n",
    "    feed_dict = {inputs: batch_inputs, labels: batch_labels}\n",
    "    sess.run(train_step, feed_dict=feed_dict)  \n",
    "        \n",
    "    if iter_train % 500 == 0 or iter_train == num_iterations_train - 1:  # Validate\n",
    "        feed_dict = {inputs: data.validation.images, labels: data.validation.labels}\n",
    "        val_summary, val_loss, val_accuracy = sess.run(\n",
    "            [validation_summaries, loss, accuracy], feed_dict=feed_dict)\n",
    "        print(\"iter {} / {}, validation accuracy = {}, loss = {}\".format(\n",
    "            iter_train, num_iterations_train, val_accuracy, val_loss))\n",
    "        \n",
    "        # Log summaries to TB (val loss, gradient norm, gradient histogram)\n",
    "        summary_writer.add_summary(val_summary, iter_train)\n",
    "        \n",
    "        # Save our session at every validation, in case our program stops.\n",
    "        saver.save(sess=sess, save_path=os.path.join(log_path, \"session\"))\n",
    "        # Let's also save the current best model somewhere else.\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            saver.save(sess=sess, save_path=os.path.join(log_path, \"best_model\", \"session\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_accuracy():\n",
    "    test_accuracies = list()\n",
    "    data.test._index_in_epoch = 0 \n",
    "    for iter_test in range(int(data.test.num_examples / batch_size)):  # Do 10k iterations of gradient descent\n",
    "        batch_inputs, batch_labels = data.test.next_batch(batch_size=batch_size, shuffle=False)\n",
    "        feed_dict = {inputs:batch_inputs, labels: batch_labels}\n",
    "        test_accuracies.append(sess.run(accuracy, feed_dict=feed_dict))\n",
    "    test_accuracy = np.mean(test_accuracies)\n",
    "    print(\"Test accuracy is: {}\".format(test_accuracy))\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is: 0.9788661599159241\n"
     ]
    }
   ],
   "source": [
    "test_accuracy_model = get_test_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restoring a saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and restoring your model parameters is pretty simple using tf.train.Saver.\n",
    "We have already created the saver object, where we provided all variables registered to the default graph. It is also possible to save only subsets of variables, such as all trainable variables (tf.GraphKeys.TRAINABLE_VARIABLES). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is: 0.09354967623949051\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer()) # Re-initialize (overwrite) model parameters\n",
    "test_accuracy_random = get_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/2018-06-23-15-53-00/session\n",
      "Test accuracy is: 0.9788661599159241\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, os.path.join(log_path, \"session\"))\n",
    "test_accuracy_model = get_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/2018-06-23-15-53-00/best_model/session\n",
      "Test accuracy is: 0.9783653616905212\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, os.path.join(log_path, \"best_model\", \"session\"))  # Load model that performed best on val set\n",
    "test_accuracy_model = get_test_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restoring the model was very simple, since our default graph still had all variables registered.\n",
    "<br>\n",
    "Otherwise, we must first build the same model with the same variable names, and then load the stored variables.\n",
    "<br>\n",
    "Try restarting the kernel of this notebook.\n",
    "Then run the code that builds the model, create the saver, and last run the previous cells to load the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
