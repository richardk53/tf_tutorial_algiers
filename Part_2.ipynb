{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Complete example with low-level APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the tutorial, we will implement a simple model, the appropriate loss function, gradient descent for optimization, and a simple training loop from scratch. \n",
    "<bl>\n",
    "In practice, you will often use higher-level APIs to do save some coding. \n",
    "But to understand what these high-level APIs do, we will first need to get our hands dirty with low-level API once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data = input_data.read_data_sets(\"data/MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is already split into training, validation and testing:\n",
      "Train:\t55000\n",
      "Val:\t5000\n",
      "Test:\t10000\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset is already split into training, validation and testing:\")\n",
    "print(\"Train:\\t{}\".format(len(data.train.labels)))\n",
    "print(\"Val:\\t{}\".format(len(data.validation.labels)))\n",
    "print(\"Test:\\t{}\".format(len(data.test.labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (42, 784)\n",
      "targets shape: (42, 10)\n"
     ]
    }
   ],
   "source": [
    "# We can extract *random* batches from the training data as follows\n",
    "batch_inputs, batch_targets = data.train.next_batch(batch_size=42)\n",
    "print(\"inputs shape: {}\".format(batch_inputs.shape))\n",
    "print(\"targets shape: {}\".format(batch_targets.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets plot a random sample input and target from the batch.\n",
    "idx = 0  \n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(batch_inputs[idx], [28, 28]), cmap='gray')\n",
    "print(\"target: {}\".format(batch_targets[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel():\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        # Note: Ignore these initializers for now, we will discuss them later when we build Neural Networks.\n",
    "        weights_initializer = tf.initializers.variance_scaling(scale=1.0, mode=\"fan_in\", distribution=\"Normal\")\n",
    "        bias_initializer = tf.initializers.constant(0.0)  # Bias can be initialized with all zeros\n",
    "\n",
    "        self.weights = tf.get_variable(shape=[dim_in, dim_out], name=\"weights\", initializer=weights_initializer)\n",
    "        self.biases = tf.get_variable(shape=[dim_out], name=\"biases\", initializer=bias_initializer)\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        outputs = tf.add(self.biases, tf.matmul(inputs, self.weights))\n",
    "        return outputs\n",
    "    \n",
    "class MLP():\n",
    "    def __init__(self, dim_in, dims_hidden, dim_out):\n",
    "        # Note: Ignore these initializers for now, we will discuss them later when we build Neural Networks.\n",
    "        weights_initializer = tf.initializers.variance_scaling(scale=1.0, mode=\"fan_in\", distribution=\"Normal\")\n",
    "        bias_initializer = tf.initializers.constant(0.0)  # Bias can be initialized with all zeros\n",
    "        \n",
    "        self.num_layers = len(dims_hidden) + 1\n",
    "        \n",
    "        dims_in = [dim_in] + list(dims_hidden)\n",
    "        dims_out = list(dims_hidden) + [dim_out]\n",
    "        self.weights, self.biases = list(), list()\n",
    "        for idx_layer, (dim_in, dim_out) in enumerate(zip(dims_in, dims_out)):\n",
    "            self.weights.append(tf.get_variable(\n",
    "                shape=[dim_in, dim_out], \n",
    "                name=\"weights_{}\".format(idx_layer), \n",
    "                initializer=weights_initializer))\n",
    "            self.biases.append(tf.get_variable(\n",
    "                shape=[dim_out], \n",
    "                name=\"biases_{}\".format(idx_layer), \n",
    "                initializer=bias_initializer\n",
    "            ))\n",
    "            \n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for idx_layer in range(self.num_layers):\n",
    "            x = tf.add(self.biases[idx_layer], tf.matmul(x, self.weights[idx_layer]))\n",
    "            if not idx_layer == self.num_layers - 1:  # last layer\n",
    "                x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images have 28*28 pixels. Targets 0-9 (categorical variable with 10 possible values)\n",
    "dim_in, dim_out = 784, 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 - Implement function**: Our model simply calculates a linear transformation of the inputs. \n",
    "The outputs can be interpreted as \"logits\", which are the logarithms of the unnormalized probabilities of our K=10 categories. \n",
    "<br>\n",
    "A common approach in (Bayesian) machine learning is to maximize the log-probability of the observed data under the model (likelihood principle). \n",
    "Equivalently, we minimize the cross-entropy between the observed and model distribution.\n",
    "<br>\n",
    "For categorical data, the data distribution is simply a dirac or \"one-hot\" distribution.\n",
    "\n",
    "To calculate the log-probabilities, you will need to\n",
    "<br>\n",
    "a) transform the output into a probability distribution \n",
    "<br>\n",
    "b) calculate the cross-entropy. \n",
    "<bl>\n",
    "**Hint**: The cross entropy (negative log-likelihood) is defined as follows:\n",
    "<br>\n",
    "$H(p_{\\text{data}}, p_{\\text{model}}) = \\sum_{k=1}^{K} p_{\\text{data}}(y_{k}) \\cdot (- \\log p_{\\text{model}}(y_k))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This shall be a task. Hide the solutions.\n",
    "# To complete both functions, you will need the following tf functions: \n",
    "# tf.cast, tf.reduce_sum, tf.argmax, tf.equal, tf.nn.softmax, tf.log\n",
    "\n",
    "def get_categorical_log_prob(logits, labels):\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    log_prob = tf.reduce_sum(labels * tf.log(probs + 1e-8), axis=-1)\n",
    "    return log_prob\n",
    "\n",
    "def get_accuracy(logits, labels):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, axis=-1), tf.argmax(labels, axis=-1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()  # Might be helpful, if you rebuild your model.\n",
    "labels = tf.placeholder(shape=[batch_size, dim_out], dtype=tf.int32, name=\"targets\")\n",
    "inputs = tf.placeholder(shape=[batch_size, dim_in], dtype=tf.float32, name=\"inputs\")\n",
    "#model = LinearTransformModel(dim_in=dim_in, dim_out=dim_out)\n",
    "model = MLP(dim_in=dim_in, dims_hidden=[128, 128], dim_out=dim_out)\n",
    "logits = model(inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = get_categorical_log_prob(logits, labels)\n",
    "loss = - tf.reduce_sum(log_prob, axis=0)  # Maximize sum of *negative* log-probabilities. \n",
    "accuracy = get_accuracy(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(loss, variables, learning_rate):\n",
    "    gradients = tf.gradients(loss, variables)\n",
    "    var_updates = []\n",
    "    for grad, var in zip(gradients, variables):\n",
    "        step = var.assign_sub(learning_rate * grad)  # GD step of one single variable\n",
    "        var_updates.append(step)\n",
    "    gradient_descent_step_op = tf.group(*var_updates)  # all GD step operations grouped into one op.\n",
    "    return gradient_descent_step_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = gradient_descent_step(loss, tf.global_variables(), 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 of 20000\n",
      "iteration 1000 of 20000\n",
      "iteration 2000 of 20000\n",
      "iteration 3000 of 20000\n",
      "iteration 4000 of 20000\n",
      "iteration 5000 of 20000\n",
      "iteration 6000 of 20000\n",
      "iteration 7000 of 20000\n",
      "iteration 8000 of 20000\n",
      "iteration 9000 of 20000\n",
      "iteration 10000 of 20000\n",
      "iteration 11000 of 20000\n",
      "iteration 12000 of 20000\n",
      "iteration 13000 of 20000\n",
      "iteration 14000 of 20000\n",
      "iteration 15000 of 20000\n",
      "iteration 16000 of 20000\n",
      "iteration 17000 of 20000\n",
      "iteration 18000 of 20000\n",
      "iteration 19000 of 20000\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = [], []\n",
    "max_iter = 20000\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) # Initializes weights and biases.\n",
    "\n",
    "for iter_train in range(max_iter):  # Do 10k iterations of gradient descent\n",
    "    batch_inputs, batch_labels = data.train.next_batch(batch_size=batch_size)\n",
    "    feed_dict = {inputs: batch_inputs, labels: batch_labels}\n",
    "    _, train_loss_iter = sess.run([train_op, loss], feed_dict=feed_dict)  \n",
    "    train_losses.append(train_loss_iter)\n",
    "    if iter_train % 1000 == 0:\n",
    "        print(\"iteration {} of {}\".format(iter_train, max_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is: 0.9786658883094788\n",
      "11.291608\n"
     ]
    }
   ],
   "source": [
    "num_samples_test = 10000\n",
    "test_losses, test_accuracies = list(), list()\n",
    "test_inputs, test_labels = data.test.images, data.test.labels\n",
    "for iter_test in range(int(num_samples_test / batch_size)):  # Do 10k iterations of gradient descent\n",
    "    batch_inputs = test_inputs[iter_test*batch_size:(iter_test+1)*batch_size]\n",
    "    batch_labels = test_labels[iter_test*batch_size:(iter_test+1)*batch_size]\n",
    "    feed_dict = {inputs:batch_inputs, labels: batch_labels}\n",
    "    test_loss_iter, test_accuracy_iter = sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "    test_losses.append(test_loss_iter)\n",
    "    test_accuracies.append(test_accuracy_iter)\n",
    "test_accuracy = np.mean(test_accuracies)\n",
    "print(\"Test accuracy is: {}\".format(test_accuracy))\n",
    "print(np.mean(test_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obrained accuracy is rather bad for this simple dataset. \n",
    "However, this was not the point of this tutorial. \n",
    "We chose a very simple model (linear) and used plain SGD for just a few (20k) training steps.\n",
    "\n",
    "We have implemented our model, loss, and optimization algorithm with the low-level tensorflow APIs. \n",
    "However, tensorflow also provides some very useful high-level APIs.\n",
    "<br>\n",
    "In the next tutorial, we will explore some of these APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
