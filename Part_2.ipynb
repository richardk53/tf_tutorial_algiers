{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Complete ML example with low-level APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the tutorial, we will train a simple parametric model on the MNIST dataset from scratch.\n",
    "\n",
    "For educational purposes, we will implement the model, its appropriate loss function, gradient descent, and a simple training loop using low-level tensorflow functions from scratch. \n",
    "<bl>\n",
    "In practice, you will often use higher-level APIs that do much of this work for you and add further functionality. \n",
    "But to understand what these high-level APIs do in the background, we will first need to get our hands dirty with low-level API and the concepts introduced in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data = input_data.read_data_sets(\"data/MNIST/\", one_hot=True)\n",
    "# Note: Currently, tf prints some annoying warnings for this dataset, just run this cell twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is already split into training, validation and testing:\n",
      "Train:\t55000\n",
      "Val:\t5000\n",
      "Test:\t10000\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset is already split into training, validation and testing:\")\n",
    "print(\"Train:\\t{}\".format(len(data.train.labels)))\n",
    "print(\"Val:\\t{}\".format(len(data.validation.labels)))\n",
    "print(\"Test:\\t{}\".format(len(data.test.labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (42, 784)\n",
      "targets shape: (42, 10)\n"
     ]
    }
   ],
   "source": [
    "# We can extract *random* batches from the training data as follows\n",
    "batch_inputs, batch_targets = data.train.next_batch(batch_size=42)\n",
    "print(\"inputs shape: {}\".format(batch_inputs.shape))\n",
    "print(\"targets shape: {}\".format(batch_targets.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets plot a random sample input and target from the batch.\n",
    "idx = 0  \n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(batch_inputs[idx], [28, 28]), cmap='gray')\n",
    "print(\"target: {}\".format(batch_targets[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel():\n",
    "    \"\"\" \n",
    "    Example class for linear model.\n",
    "    Creates the model parameters (tf.Variable) when class is instantiated. \n",
    "    Builds the graph when the instance is applied to an input.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self.weights = tf.Variable(initial_value=tf.random_normal([dim_in, dim_out], stddev=0.1), name=\"weights\")\n",
    "        self.biases = tf.Variable(initial_value=tf.zeros([dim_out]), name=\"biases\")\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        outputs = tf.add(self.biases, tf.matmul(inputs, self.weights))\n",
    "        return outputs\n",
    "\n",
    "MyModel = LinearModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images have 28*28 pixels. Targets 0-9 (categorical variable with 10 possible values)\n",
    "dim_in, dim_out = 784, 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()  # Might be helpful, if you rebuild your model.\n",
    "\n",
    "labels = tf.placeholder(shape=[batch_size, dim_out], dtype=tf.int32, name=\"targets\")\n",
    "inputs = tf.placeholder(shape=[batch_size, dim_in], dtype=tf.float32, name=\"inputs\")\n",
    "\n",
    "model = MyModel(dim_in=dim_in, dim_out=dim_out)\n",
    "outputs = model(inputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 - Implement loss function**: Our model simply calculates a linear transformation of the inputs. \n",
    "The outputs can be interpreted as \"logits\", which are the logarithms of the unnormalized probabilities of our K=10 classes. \n",
    "<br>\n",
    "One common approach in machine learning is to maximize the likelihood (maximum likelihood), which is the log-probability of the observed data under the model:\n",
    "<br>\n",
    "$\\hat{\\theta} = \\text{argmax}_{\\theta} \\sum_{n=1}^{N} \\log p_{\\theta}(\\mathbf{y}^{(n)} ~|~ \\mathbf{x}^{(n)})$, where $\\theta$ are the model parameters, $\\mathbf{y}$ are labels, $\\mathbf{x}$ are inputs and N is the number of samples in the dataset. \n",
    "When training with *stochastic* gradient descent, we approximate the sum using a mini-batch of samples instead of the whole dataset.\n",
    "<br>\n",
    "In case of classification, the likelihood $p_{\\theta}(\\mathbf{y}^{(n)} ~|~ \\mathbf{x}^{(n)})$ is a multinomial distribution:\n",
    "<br>\n",
    "\\begin{equation}\n",
    "p_{\\theta}(\\mathbf{y}^{(n)} ~|~ \\mathbf{x}^{(n)}) = \\prod_{k=1}^{K} \\big[ \\text{softmax}(f_{\\theta}(\\mathbf{x}^{(n)}))_{k} \\big]^{\\mathbf{y}^{(n)}_{k}},\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\log p_{\\theta}(\\mathbf{y}^{(n)} ~|~ \\mathbf{x}^{(n)}) = \\sum_{k=1}^{K} \\mathbf{y}^{(n)}_{k} \\cdot \\log \\text{softmax}(f_{\\theta}(\\mathbf{x}^{(n)}))_{k},\n",
    "\\end{equation}\n",
    "where $\\mathbf{y}^{(n)}_{k}$ is a discrete variable that takes 0 or 1.\n",
    "\n",
    "**Hints**: <bl>\n",
    "To calculate the log-likelihood, you will need to <br>\n",
    "a) transform the model output (logits) into a probability distribution.\n",
    "<br>\n",
    "b) calculate the log_probability of each sample $\\mathbf{y^{(n)}}$ under this distribution.\n",
    "<br>\n",
    "c) sum or average the per-sample log-probs over your batch of independent samples.    \n",
    "<bl>\n",
    "**Task 2 - Implement accuracy**: The accuracy is the percentage of correctly classified classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete these functions, you might need the following tf functions: \n",
    "# tf.cast, tf.reduce_sum, tf.argmax, tf.equal, tf.nn.softmax, tf.log\n",
    "def get_categorical_log_prob(logits, labels):\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    raise NotImplementedError(\"Please implement this function.\")\n",
    "\n",
    "def get_accuracy(logits, labels):\n",
    "    raise NotImplementedError(\"Please implement this function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = get_categorical_log_prob(logits=outputs, labels=labels)\n",
    "loss = - log_prob\n",
    "accuracy = get_accuracy(logits=outputs, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(loss, variables, learning_rate):\n",
    "    gradients = tf.gradients(loss, variables)\n",
    "    var_updates = []\n",
    "    for grad, var in zip(gradients, variables):\n",
    "        step = var.assign_sub(learning_rate * grad)  # GD step of one single variable\n",
    "        var_updates.append(step)\n",
    "    gradient_descent_step_op = tf.group(*var_updates)  # all GD step operations grouped into one op.\n",
    "    return gradient_descent_step_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = gradient_descent_step(loss, tf.global_variables(), 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 of 20000\n",
      "iteration 1000 of 20000\n",
      "iteration 2000 of 20000\n",
      "iteration 3000 of 20000\n",
      "iteration 4000 of 20000\n",
      "iteration 5000 of 20000\n",
      "iteration 6000 of 20000\n",
      "iteration 7000 of 20000\n",
      "iteration 8000 of 20000\n",
      "iteration 9000 of 20000\n",
      "iteration 10000 of 20000\n",
      "iteration 11000 of 20000\n",
      "iteration 12000 of 20000\n",
      "iteration 13000 of 20000\n",
      "iteration 14000 of 20000\n",
      "iteration 15000 of 20000\n",
      "iteration 16000 of 20000\n",
      "iteration 17000 of 20000\n",
      "iteration 18000 of 20000\n",
      "iteration 19000 of 20000\n"
     ]
    }
   ],
   "source": [
    "num_iterations_train = 20000\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) # Initializes weights and biases.\n",
    "\n",
    "for iter_train in range(num_iterations_train):\n",
    "    batch_inputs, batch_labels = data.train.next_batch(batch_size=batch_size)\n",
    "    sess.run(train_op, feed_dict={inputs: batch_inputs, labels: batch_labels})  \n",
    "    if iter_train % 1000 == 0:\n",
    "        print(\"iteration {} of {}\".format(iter_train, num_iterations_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is: 0.9253805875778198\n",
      "34.159634\n"
     ]
    }
   ],
   "source": [
    "num_samples_test = 10000\n",
    "test_losses, test_accuracies = list(), list()\n",
    "test_inputs, test_labels = data.test.images, data.test.labels\n",
    "for iter_test in range(int(num_samples_test / batch_size)):  # Go through test set once\n",
    "    # Prepare feed_dict with a batch of inputs and targets\n",
    "    batch_inputs = test_inputs[iter_test*batch_size:(iter_test+1)*batch_size]\n",
    "    batch_labels = test_labels[iter_test*batch_size:(iter_test+1)*batch_size]\n",
    "    feed_dict = {inputs:batch_inputs, labels: batch_labels}\n",
    "    # Evaluate loss and accuracy\n",
    "    test_loss_iter, test_accuracy_iter = sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "    # Append to list of losses and accuracies\n",
    "    test_losses.append(test_loss_iter)\n",
    "    test_accuracies.append(test_accuracy_iter)\n",
    "test_accuracy = np.mean(test_accuracies)\n",
    "print(\"Test accuracy is: {}\".format(test_accuracy))\n",
    "print(np.mean(test_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 - Implement an Multi-layer Perceptron:\n",
    "The obtained accuracy is rather bad for this simple dataset. \n",
    "We chose a very simple model (linear) and used plain SGD for relatively few (20k) training steps.\n",
    "<br>\n",
    "Please change the linear model to an MLP, e.g. with two hidden layers, each with 128 units and tanh nonlinearities. Do not use tf.keras.layers, tf.layers or similar high-level APIs.\n",
    "<br>\n",
    "**Hint:** Each layer $l$ of an MLP simply computes a linear transformation with subsequent non-linearity:\n",
    "<br>\n",
    "$h_{l} = f(W_{l} * h_{l-1} + b_{l})$, where $h_{0} = x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
