{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \"\"\" Simple MLP with tanh activations \"\"\"\n",
    "    def __init__(self, dim_in, dim_out, dims_hidden=[128, 128]):        \n",
    "        self.num_layers = len(dims_hidden) + 1  # + 1 for the linear output layer\n",
    "        \n",
    "        dims_in = [dim_in] + list(dims_hidden)  # [dim_in, hidden_1, hidden_2]\n",
    "        dims_out = list(dims_hidden) + [dim_out]  # [hidden_1, hidden_2, dim_out]\n",
    "        \n",
    "        self.weights, self.biases = list(), list()\n",
    "        for idx_layer, (d_in, d_out) in enumerate(zip(dims_in, dims_out)):\n",
    "            # Here we initialize the weights from a Normal distribution (diagonal) with std = 0.1.\n",
    "            # Note that there are heuristics that work better. \n",
    "            # We will use them in the next part of the tutorial.\n",
    "            weight = tf.Variable(initial_value=tf.random_normal([d_in, d_out], stddev=0.1), \n",
    "                                 name=\"weights_{}\".format(idx_layer))\n",
    "            bias = tf.Variable(initial_value=tf.zeros([d_out]), \n",
    "                               name=\"biases_{}\".format(idx_layer))\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "            \n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for idx_layer in range(self.num_layers):\n",
    "            x = tf.add(self.biases[idx_layer], tf.matmul(x, self.weights[idx_layer]))\n",
    "            if not idx_layer == self.num_layers - 1:  # last layer\n",
    "                x = tf.tanh(x)\n",
    "        return x\n",
    "\n",
    "MyModel = MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_log_prob(logits, labels):\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    log_probs_per_sample = tf.reduce_sum(labels * tf.log(probs + 1e-8), axis=-1)  # small constant for numerical stability of log\n",
    "    log_prob_batch = tf.reduce_sum(log_probs_per_sample, axis=0)\n",
    "    # log_prob_batch = tf.reduce_sum(labels * tf.log(probs + 1e-8))  # Could sum directly over both axes.\n",
    "    return log_prob_batch\n",
    "\n",
    "def get_accuracy(logits, labels):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, axis=-1), tf.argmax(labels, axis=-1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
